---
title: "Week 1 - Swiftkey"
output: 
  html_notebook:
    code_folding: hide
---

## Environment

```{r env, message=FALSE, warning=FALSE, echo=FALSE}
enviro<- c("rvest", "tidytext", "tm", "tidyverse", "NLP", 
           "knitr", "readr", "openNLP", "glue", "RWeka", "stringi",    
           "SnowballC", "wordcloud2")

for (i in 1:length(enviro)){
library(enviro[i], character.only = TRUE)
}

```

## Data Loading

```{r download}
# if (!file.exists("data")) {
#        dir.create("data")}
# download.file("https://d396qusza40orc.cloudfront.net/ds# scapstone/dataset/Coursera-SwiftKey.zip", destfile = "./data/Coursera-SwiftKey.zip")
# unzip("./data/Coursera-SwiftKey.zip", exdir = "./data")
```



```{r combining}
files2 <- list.dirs("./data/final")
lsfile <-  paste0(files2[2:5],"/",
                  list.files(files2[2:5]))
ldir <- normalizePath(files2[2:5], "rb") # gives us the directory path
finaldir <- dir(path=ldir, full.names=TRUE) # gives us full path and filename
# fulltext <- lapply(finaldir,readLines)
```


```{r buildtable, message=FALSE, warning=FALSE, results='hide'}
## Num_Words total number of words in a txt file
## Mean_Words average word per line per txt file
## Num_Lines number of lines per txt file
Num_Words <- vector("numeric")
Mean_Words <- vector("numeric")
Num_Lines <- vector("numeric")
Min_Words <- vector("numeric")
Max_Words <- vector("numeric")
for(i in 1:12){
Num_Words[i] <- print(sum(stri_count_words(readLines(finaldir[[i]]))))
Mean_Words[i] <- print(round(mean(stri_count_words(readLines(finaldir[[i]])))),digits=2)
Min_Words[i] <- print(round(min(stri_count_words(readLines(finaldir[[i]])))),digits=2)
Max_Words[i] <- print(round(max(stri_count_words(readLines(finaldir[[i]])))),digits=2)
Num_Lines[i] <- print(length(readLines(finaldir[i])))
}

```


```{r finaltable}
list_files <- tibble('Name' = list.files(files2[2:5]), 
                     'Size_MB' = round(file.size(finaldir)/10^6,digits =2),
                     Lines = Num_Lines, Words = Num_Words, Min =Min_Words,
                     Average= Mean_Words, Max=Max_Words)
kable(list_files, align = c(rep('c', times= 5)))
```

```{r SourceData}
source_data <- DirSource(ldir)

corpus_data <- VCorpus(source_data)

summary(corpus_data)
inspect(corpus_data[5])
meta(corpus_data[[5]], "id")
```


```{r transformation}
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]","",x)
getTransformations()

us_news <- tm_map(corpus_data[5], removePunctuation)

us_news <- tm_map(us_news, removeNumbers)
us_news <- tm_map(us_news, content_transformer(tolower))
us_news <- tm_map(us_news, removeWords, stopwords(kind = "en"))
us_news <- tm_map(us_news, stripWhitespace)
us_news <- tm_map(us_news, stemDocument)
us_news <- tm_map(us_news, content_transformer(removeSpecialChars))
us_news_trix <- DocumentTermMatrix(us_news)

```

```{r frequency}
inspect(us_news_trix)
findFreqTerms(us_news_trix, 25)
findAssocs(us_news_trix, "feet", .1)
inspect(removeSparseTerms(us_news_trix,.4))
Terms(us_news_trix)
freqq <- colSums(as.matrix(us_news_trix))
```

```{r}

View(reuters)
```


```{r SentimentScore}
wf <- data.frame(word=txtmatx[["dimnames"]][["Terms"]], freq=txtmatx[["v"]])

(wf$freq)

```


```{r}

wordcloud2(d_Rcran)
```
```{r}
Rcran_list <- findFreqTerms(doc_mat, lowfreq = 10)
findFreqTerms(doc_mat, lowfreq = 10)
```

```{r}
findAssocs(doc_mat, terms = "say", corlimit = 0.05)
```


```{r}
barplot(d_Rcran[1:25,]$freq, 
        las = 2, 
        names.arg = d_Rcran[1:25,]$word,
        col ="lightyellow", 
        main ="Most Frequent Words From R-cran Packages",
        ylab = "Word Count")
```

